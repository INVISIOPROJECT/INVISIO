# -*- coding: utf-8 -*-
"""topic_modeling_pipeline (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vR2CxsgiLqc-6MV6WpA6EltGinHkHHSB

# خط أنابيب Topic Modeling

هذا النوتبوك يحتوي على الخطوات الكاملة من تحميل البيانات إلى استخراج المواضيع باستخدام Word2Vec، BERT، وتقنيات تقليل الأبعاد والتجميع.
كل خلية تشمل شروحات قصيرة بالعربية لسهولة التتبع.
"""

!pip install --upgrade \
    gensim transformers torch torchvision torchaudio \
    numpy==1.25.1 scikit-learn umap-learn matplotlib bertopic

import logging
import os
import numpy as np
import pandas as pd
import torch
import matplotlib.pyplot as plt
from gensim.models import Word2Vec
from transformers import BertTokenizer, BertModel
from sklearn.decomposition import PCA
import umap
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from bertopic import BERTopic

# إعداد سجل الأحداث لتتبع العمليات
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

"""## دوال المعالجة

في هذه الخلية نعرّف الدوال اللازمة لتحميل البيانات، تنظيفها، استخراج التضمينات، تقليل الأبعاد، والتجميع.

"""

def load_texts(path):
    ext = os.path.splitext(path)[1].lower()
    if ext == '.csv':
        df = pd.read_csv(path)
        return df[df.columns[0]].astype(str).tolist()
    else:
        with open(path, encoding='utf-8') as f:
            return [line.strip() for line in f if line.strip()]

def preprocess(texts):
    # تنظيف وتقسيم النصوص إلى كلمات
    return [[w.lower() for w in doc.split() if w.isalpha()] for doc in texts]

def train_word2vec(sentences, **kwargs):
    # تدريب نموذج Word2Vec
    model = Word2Vec(sentences=sentences, **kwargs)
    return model

def get_bert_embeddings(texts, model_name='bert-base-uncased', device=None, batch_size=32):
    # استخراج تضمينات BERT بجعل المتوسط عبر الكلمات
    device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
    tokenizer = BertTokenizer.from_pretrained(model_name)
    model = BertModel.from_pretrained(model_name).to(device).eval()
    embs = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        with torch.no_grad():
            out = model(**inputs)
        embs.append(out.last_hidden_state.mean(dim=1).cpu().numpy())
    return np.vstack(embs)

def reduce_embeddings(embs, method='umap', **kwargs):
    # تقليل الأبعاد باستخدام PCA أو UMAP
    if method == 'pca':
        pca = PCA(n_components=2, whiten=True, random_state=42)
        return pca.fit_transform(embs)
    elif method == 'umap':
        reducer = umap.UMAP(n_components=2, random_state=42, **kwargs)
        return reducer.fit_transform(embs)
    else:
        raise ValueError("Method must be 'pca' or 'umap'")

def cluster_and_plot(X, methods=('kmeans','dbscan','agglo'), k=5):
    # تجميع وعرض النتائج بيانيًا
    fig, axes = plt.subplots(1, len(methods), figsize=(5*len(methods),4))
    for ax, m in zip(axes, methods):
        if m == 'kmeans':
            labels = KMeans(n_clusters=k, random_state=42).fit_predict(X)
            title = 'KMeans'
        elif m == 'dbscan':
            labels = DBSCAN(eps=0.5, min_samples=5).fit_predict(X)
            title = 'DBSCAN'
        elif m == 'agglo':
            labels = AgglomerativeClustering(n_clusters=k).fit_predict(X)
            title = 'Agglomerative'
        ax.scatter(X[:,0], X[:,1], c=labels, s=10)
        ax.set_title(title)
    plt.tight_layout()
    plt.show()

def build_vectorizers():
    # بناء ناقلات BoW و TF-IDF
    unigram = CountVectorizer(stop_words='english', token_pattern=r'\b\w+\b', max_features=5000)
    ngram   = CountVectorizer(stop_words=None, ngram_range=(2,3), max_features=3000)
    tfidf   = TfidfVectorizer(stop_words='english', ngram_range=(1,2), max_df=0.8, min_df=5, max_features=4000)
    return unigram, ngram, tfidf

def compute_ctfidf(docs, stop_words=None):
    # حساب C-TF-IDF
    vect = CountVectorizer(stop_words=stop_words or 'english', min_df=1)
    X = vect.fit_transform(docs).toarray()
    tf = X
    A  = tf.sum(axis=1).mean()
    idf = np.log(1 + A / (tf.sum(axis=0) + 1e-9))
    ctfidf = tf * idf
    terms = vect.get_feature_names_out()
    return ctfidf, terms

def topic_modeling_bertopic(texts, embeddings=None, nr_topics='auto'):
    # تطبيق BERTopic لاستخراج المواضيع
    model = BERTopic(nr_topics=nr_topics, verbose=True)
    topics, probs = model.fit_transform(texts, embeddings)
    return model, topics, probs

"""## التنفيذ الرئيسي

نقوم بتحميل البيانات، تنظيفها، تدريب Word2Vec، استخراج تضمينات BERT، تقليل الأبعاد، وتجميع البيانات، وأخيرًا استخراج المواضيع باستخدام BERTopic.

"""

# تحميل البيانات
data_path = 'reuters_headlines.csv'  # غيّر المسار عند الحاجة
texts = load_texts(data_path)
print(f"عدد الوثائق: {len(texts)}")

# تنظيف النصوص وإعدادها لـ Word2Vec
sentences = preprocess(texts)

# تدريب نموذج Word2Vec
w2v = train_word2vec(sentences, vector_size=100, window=5, min_count=2, workers=4, epochs=10)
print("Word2Vec حجم المفردات =", len(w2v.wv))

# استخراج تضمينات BERT
embs = get_bert_embeddings(texts)
print("شكل التضمينات:", embs.shape)

# حفظ التضمينات
np.save("bert_embeddings.npy", embs)
w2v.save("word2vec.model")

# تقليل الأبعاد
X_pca  = reduce_embeddings(embs, method='pca')
X_umap = reduce_embeddings(embs, method='umap', n_neighbors=15, min_dist=0.1)
print("الأشكال بعد التقليل:", X_pca.shape, X_umap.shape)

# عرض التجميع
cluster_and_plot(X_umap)

# بناء الناقلات وحساب الأشكال
uni, ng, tf = build_vectorizers()
X_uni = uni.fit_transform(texts)
X_ng  = ng.fit_transform(texts)
X_tfidf = tf.fit_transform(texts)
print("أشكال الناقلات:", X_uni.shape, X_ng.shape, X_tfidf.shape)

# حساب C-TF-IDF
docs_combined = [" ".join(texts)]
ctfidf, terms = compute_ctfidf(docs_combined)
top_idx = np.argsort(ctfidf[0])[::-1][:20]
print("أهم المصطلحات:", [terms[i] for i in top_idx])

# استخراج المواضيع باستخدام BERTopic
bert_model, topics, probs = topic_modeling_bertopic(texts, embeddings=embs)
info = bert_model.get_topic_info()
print(info.head())

import hdbscan

# تحميل البيانات
data_path = 'reuters_headlines.csv'  # غيّر المسار عند الحاجة
texts = load_texts(data_path)
print(f"عدد الوثائق: {len(texts)}")

# تنظيف النصوص وإعدادها لـ Word2Vec
sentences = preprocess(texts)

# تدريب نموذج Word2Vec
w2v = train_word2vec(sentences, vector_size=100, window=5, min_count=2, workers=4, epochs=10)
print("Word2Vec حجم المفردات =", len(w2v.wv))

# استخراج تضمينات BERT
embs = get_bert_embeddings(texts)
print("شكل التضمينات:", embs.shape)

# حفظ التضمينات
np.save("bert_embeddings.npy", embs)
w2v.save("word2vec.model")

# بعد ما ننزل الأبعاد (مثلاً X_umap):
X_pca  = reduce_embeddings(embs, method='pca')
print("أشكال بعد التقليل:", X_pca.shape)

# استخدام HDBSCAN للتجميع
clusterer = hdbscan.HDBSCAN(min_cluster_size=5,  # أقل حجم عنقود مسموح، جرب تغيرها إذا ما عجبك الناتج
                            metric='euclidean',
                            cluster_selection_method='eom')
labels = clusterer.fit_predict(X_pca)

# طباعة كم عنقود طلع وبنرسمه
n_clusters = len(set(labels) - {-1})  # -1 هي النقاط الشاذة
print(f"عدد العناقيد المكتشفة (بدون الشوائب): {n_clusters}")
print(f"عدد النقاط المسمّاة 'شاذة': {(labels == -1).sum()}")

# رسم النتائج
import matplotlib.pyplot as plt

plt.figure(figsize=(6, 5))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='Spectral', s=10)
plt.title("HDBSCAN Results on PCA data")
plt.xlabel("first")
plt.ylabel("second")
plt.colorbar(label='Cluster Label')
plt.show()

# تحميل البيانات
data_path = 'cnbc_headlines.csv'  # غيّر المسار عند الحاجة
texts = load_texts(data_path)
print(f"عدد الوثائق: {len(texts)}")

# تنظيف النصوص وإعدادها لـ Word2Vec
sentences = preprocess(texts)

# تدريب نموذج Word2Vec
w2v = train_word2vec(sentences, vector_size=100, window=5, min_count=2, workers=4, epochs=10)
print("Word2Vec حجم المفردات =", len(w2v.wv))

# استخراج تضمينات BERT
embs = get_bert_embeddings(texts)
print("شكل التضمينات:", embs.shape)

# حفظ التضمينات
np.save("bert_embeddings.npy", embs)
w2v.save("word2vec.model")

# تقليل الأبعاد
X_pca  = reduce_embeddings(embs, method='pca')
X_umap = reduce_embeddings(embs, method='umap', n_neighbors=15, min_dist=0.1)
print("الأشكال بعد التقليل:", X_pca.shape, X_umap.shape)

# عرض التجميع
cluster_and_plot(X_umap)

# بناء الناقلات وحساب الأشكال
uni, ng, tf = build_vectorizers()
X_uni = uni.fit_transform(texts)
X_ng  = ng.fit_transform(texts)
X_tfidf = tf.fit_transform(texts)
print("أشكال الناقلات:", X_uni.shape, X_ng.shape, X_tfidf.shape)

# حساب C-TF-IDF
docs_combined = [" ".join(texts)]
ctfidf, terms = compute_ctfidf(docs_combined)
top_idx = np.argsort(ctfidf[0])[::-1][:20]
print("أهم المصطلحات:", [terms[i] for i in top_idx])

# استخراج المواضيع باستخدام BERTopic
bert_model, topics, probs = topic_modeling_bertopic(texts, embeddings=embs)
info = bert_model.get_topic_info()
print(info.head())

import hdbscan

# تحميل البيانات
data_path = 'cnbc_headlines.csv'  # غيّر المسار عند الحاجة
texts = load_texts(data_path)
print(f"عدد الوثائق: {len(texts)}")

# تنظيف النصوص وإعدادها لـ Word2Vec
sentences = preprocess(texts)

# تدريب نموذج Word2Vec
w2v = train_word2vec(sentences, vector_size=100, window=5, min_count=2, workers=4, epochs=10)
print("Word2Vec حجم المفردات =", len(w2v.wv))

# استخراج تضمينات BERT
embs = get_bert_embeddings(texts)
print("شكل التضمينات:", embs.shape)

# حفظ التضمينات
np.save("bert_embeddings.npy", embs)
w2v.save("word2vec.model")

# بعد ما ننزل الأبعاد (مثلاً X_umap):
X_pca  = reduce_embeddings(embs, method='pca')
print("أشكال بعد التقليل:", X_pca.shape)

# استخدام HDBSCAN للتجميع
clusterer = hdbscan.HDBSCAN(min_cluster_size=5,  # أقل حجم عنقود مسموح، جرب تغيرها إذا ما عجبك الناتج
                            metric='euclidean',
                            cluster_selection_method='eom')
labels = clusterer.fit_predict(X_pca)

# طباعة كم عنقود طلع وبنرسمه
n_clusters = len(set(labels) - {-1})  # -1 هي النقاط الشاذة
print(f"عدد العناقيد المكتشفة (بدون الشوائب): {n_clusters}")
print(f"عدد النقاط المسمّاة 'شاذة': {(labels == -1).sum()}")

# رسم النتائج
import matplotlib.pyplot as plt

plt.figure(figsize=(6, 5))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='Spectral', s=10)
plt.title("HDBSCAN Results on PCA data")
plt.xlabel("first")
plt.ylabel("second")
plt.colorbar(label='Cluster Label')
plt.show()

def load_texts(path, column_name=None):
    ext = os.path.splitext(path)[1].lower()
    if ext == '.csv':
        df = pd.read_csv(path)
        if column_name and column_name in df.columns:
            return df[column_name].astype(str).tolist()
        else:
            # إذا لم يمرر اسم العمود، نأخذ العمود الأول
            return df[df.columns[0]].astype(str).tolist()
    else:
        with open(path, encoding='utf-8') as f:
            return [line.strip() for line in f if line.strip()]

# تحميل البيانات
texts = load_texts('guardian_headlines.csv', column_name='Headlines')
print(f"عدد الوثائق: {len(texts)}")

# تنظيف النصوص وإعدادها لـ Word2Vec
sentences = preprocess(texts)

# تدريب نموذج Word2Vec
w2v = train_word2vec(sentences, vector_size=100, window=5, min_count=2, workers=4, epochs=10)
print("Word2Vec حجم المفردات =", len(w2v.wv))

# استخراج تضمينات BERT
embs = get_bert_embeddings(texts)
print("شكل التضمينات:", embs.shape)

# حفظ التضمينات
np.save("bert_embeddings.npy", embs)
w2v.save("word2vec.model")

# تقليل الأبعاد
X_pca  = reduce_embeddings(embs, method='pca')
X_umap = reduce_embeddings(embs, method='umap', n_neighbors=15, min_dist=0.1)
print("الأشكال بعد التقليل:", X_pca.shape, X_umap.shape)

# عرض التجميع
cluster_and_plot(X_umap)

# بناء الناقلات وحساب الأشكال
uni, ng, tf = build_vectorizers()
X_uni = uni.fit_transform(texts)
X_ng  = ng.fit_transform(texts)
X_tfidf = tf.fit_transform(texts)
print("أشكال الناقلات:", X_uni.shape, X_ng.shape, X_tfidf.shape)

# حساب C-TF-IDF
docs_combined = [" ".join(texts)]
ctfidf, terms = compute_ctfidf(docs_combined)
top_idx = np.argsort(ctfidf[0])[::-1][:20]
print("أهم المصطلحات:", [terms[i] for i in top_idx])

# استخراج المواضيع باستخدام BERTopic
bert_model, topics, probs = topic_modeling_bertopic(texts, embeddings=embs)
info = bert_model.get_topic_info()
print(info.head())

!pip install hdbscan

# تحميل البيانات
texts = load_texts('guardian_headlines.csv', column_name='Headlines')
texts = load_texts(data_path)
print(f"عدد الوثائق: {len(texts)}")

# تنظيف النصوص وإعدادها لـ Word2Vec
sentences = preprocess(texts)

# تدريب نموذج Word2Vec
w2v = train_word2vec(sentences, vector_size=100, window=5, min_count=2, workers=4, epochs=10)
print("Word2Vec حجم المفردات =", len(w2v.wv))

# استخراج تضمينات BERT
embs = get_bert_embeddings(texts)
print("شكل التضمينات:", embs.shape)

# حفظ التضمينات
np.save("bert_embeddings.npy", embs)
w2v.save("word2vec.model")

# بعد ما ننزل الأبعاد (مثلاً X_umap):
X_pca  = reduce_embeddings(embs, method='pca')
print("أشكال بعد التقليل:", X_pca.shape)

# استخدام HDBSCAN للتجميع
clusterer = hdbscan.HDBSCAN(min_cluster_size=5,  # أقل حجم عنقود مسموح، جرب تغيرها إذا ما عجبك الناتج
                            metric='euclidean',
                            cluster_selection_method='eom')
labels = clusterer.fit_predict(X_pca)

# طباعة كم عنقود طلع وبنرسمه
n_clusters = len(set(labels) - {-1})  # -1 هي النقاط الشاذة
print(f"عدد العناقيد المكتشفة (بدون الشوائب): {n_clusters}")
print(f"عدد النقاط المسمّاة 'شاذة': {(labels == -1).sum()}")

# رسم النتائج
import matplotlib.pyplot as plt

plt.figure(figsize=(6, 5))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='Spectral', s=10)
plt.title("HDBSCAN Results on PCA data")
plt.xlabel("first")
plt.ylabel("second")
plt.colorbar(label='Cluster Label')
plt.show()

